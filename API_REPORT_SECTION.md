# API / Model Serving Layer

## Objective

The API layer exposes the trained Avazu click-through rate (CTR) prediction model via a REST interface, providing stateless inference serving for production deployment. The implementation maintains clear separation from training pipelines and MLflow tracking, ensuring the serving component can operate independently while consuming pre-trained model artifacts.

## Technology Stack and Design Choices

The serving layer is built using FastAPI, chosen for its automatic OpenAPI documentation, type validation via Pydantic, and asynchronous request handling capabilities. Model artifacts are loaded using Joblib, which serializes the trained model, feature hasher, and preprocessing configuration into a single file. The model path is configurable via the `MODEL_PATH` environment variable, enabling model replacement without code changes—critical for A/B testing and model versioning in production.

## Endpoint Design

The API provides two endpoints. The `GET /health` endpoint returns service status, enabling health checks and monitoring integration. The `POST /predict` endpoint accepts feature data as key-value pairs in the request body and returns both the click probability (float) and binary click prediction (integer). The binary prediction applies a 0.5 threshold to the probability score, mapping values ≥0.5 to class 1 (click) and <0.5 to class 0 (no click).

## Model Loading and Inference Flow

At application startup, the model artifact is loaded once from disk and stored in memory, minimizing latency during inference. The `build_predictor` function encapsulates the preprocessing and prediction logic, extracting the feature hasher, feature cross configuration, and model(s) from the artifact. During inference, incoming feature dictionaries are converted to token dictionaries using the same preprocessing pipeline as training, then transformed via the hasher into a sparse feature vector. The model's `predict_proba` method generates the click probability, which is thresholded to produce the binary prediction.

## Preprocessing Consistency

The API preserves the same hashed feature representation and optional feature crosses used during training to ensure consistency between training and serving. The `to_feature_dict` utility function generates token dictionaries in the format expected by the feature hasher, maintaining identical tokenization logic (including feature cross generation) as the training pipeline. This ensures that the same feature engineering applied during model training is replicated at inference time.

## Robustness

Request validation is enforced through Pydantic models (`PredictRequest` and `PredictResponse`), automatically rejecting malformed inputs and providing clear error messages. Exception handling wraps the prediction logic, converting runtime errors into appropriate HTTP 400 responses with descriptive details. The health endpoint enables integration with orchestration systems and monitoring tools for automated service health verification.

---

## Screenshots to Include in the Report

**Screenshot 1: Swagger UI Documentation**
- **What to capture:** Open the browser to `http://localhost:8000/docs` (or your server URL) showing the FastAPI Swagger UI interface with the `/predict` endpoint visible.
- **What this proves:** Demonstrates the automatic API documentation generated by FastAPI, showing the endpoint structure, request/response schemas, and interactive testing capability.

**Screenshot 2: Successful Prediction Request and Response**
- **What to capture:** Use the Swagger UI "Try it out" feature or Postman to send a POST request to `/predict` with sample feature data, showing both the request payload and the successful response containing `click_probability` and `click_prediction`.
- **What this proves:** Validates that the API correctly processes feature inputs, executes inference, and returns predictions in the expected format.

**Screenshot 3: FastAPI Server Running**
- **What to capture:** Terminal/command prompt showing the uvicorn command starting the FastAPI server (e.g., `uvicorn src.serving.app:app --reload`) with startup logs and the server listening on a port.
- **What this proves:** Confirms the server deployment process and shows the application is running and ready to accept requests.

**Screenshot 4: Environment Variable Configuration (Optional)**
- **What to capture:** Terminal showing the `MODEL_PATH` environment variable being set or a `.env` file containing `MODEL_PATH=...`, or code/config showing the environment variable usage.
- **What this proves:** Demonstrates the configurable model path design, enabling model replacement without code changes.

